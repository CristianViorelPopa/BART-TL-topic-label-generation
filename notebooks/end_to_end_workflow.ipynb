{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End-to-end Training of BART-TL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through fine-tuning a BART-TL model according to this paper: https://www.aclweb.org/anthology/2021.eacl-main.121.pdf.\n",
    "\n",
    "The models showcased there are already available on Huggingface if you need a quick way of generating labels:\n",
    "- https://huggingface.co/cristian-popa/bart-tl-all\n",
    "- https://huggingface.co/cristian-popa/bart-tl-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.0.1 in /usr/local/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/site-packages (from gensim==4.0.1->-r ../requirements.txt (line 1)) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim==4.0.1->-r ../requirements.txt (line 1)) (1.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Applying LDA on corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is extracting topics from the StackExchange corpora. In this notebook we only experiment with the biology corpus. Using the other corpora is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CORPUS_FILE=../corpus/biology.stackexchange.com/Posts.xml\n",
      "env: LDA_INFO_PATH=../experiment/lda_info/\n",
      "env: TOPICS_PATH=../experiment/topics/\n",
      "env: NETL_LABELS_PATH=../experiment/netl_labels/\n",
      "env: BART_DATASET_PATH=../experiment/dataset_fairseq/\n",
      "env: HUGGINGFACE_MODEL_SAVE_PATH=../experiment/bart-tl-all/\n"
     ]
    }
   ],
   "source": [
    "# XML file used in the experiment\n",
    "%env CORPUS_FILE=../corpus/biology.stackexchange.com/Posts.xml\n",
    "# directory where the LDA models and info will be stored\n",
    "%env LDA_INFO_PATH=../experiment/lda_info/\n",
    "# directory where the topic data will be stored\n",
    "%env TOPICS_PATH=../experiment/topics/\n",
    "# directory where the NETL-extracted labels will be stored\n",
    "%env NETL_LABELS_PATH=../experiment/netl_labels/\n",
    "\n",
    "# directory where the dataset for fine-tuning BART will be stored\n",
    "%env BART_DATASET_PATH=../experiment/dataset_fairseq/\n",
    "# where the model will be saved\n",
    "%env HUGGINGFACE_MODEL_SAVE_PATH=../experiment/bart-tl-all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ${LDA_INFO_PATH}\n",
    "!mkdir -p ${TOPICS_PATH}\n",
    "!mkdir -p ${NETL_LABELS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the script that applies LDA on the biology corpus from StackExchange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Done pre-processing documents\n",
      "Done pre-processing corpus\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python3 ../lda/apply_lda.py \\\n",
    "    --input-file ${CORPUS_FILE} \\\n",
    "    --output-prefix ${LDA_INFO_PATH}/biology \\\n",
    "    --topics-prefix ${TOPICS_PATH}/biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the `../experiment/lda_info` directory should contain the following files:\n",
    "- `biology.dct`\n",
    "- `biology.model`\n",
    "- `biology.model.expElogbeta.npy`\n",
    "- `biology.model.id2word`\n",
    "- `biology.model.state`\n",
    "- `biology_corpus.pickle`\n",
    "\n",
    "These files are useful for extracting the noun phrases at step 3 and are saved so you can better inspect LDA-related issues for experiments.\n",
    "\n",
    "The `../experiment/topics` directory will contain:\n",
    "- `biology.csv`\n",
    "- `biology.json`\n",
    "- `biology_sentences.txt`\n",
    "- `biology_sentences_raw.txt`\n",
    "\n",
    "These are files that will be used to generate datasets the BART model will be fine-tuned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining NETL labels for topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the topics from the corpus, these need to be labeled using the NETL method (https://github.com/sb1992/NETL-Automatic-Topic-Labelling-).\n",
    "\n",
    "The original process proposed by the authors was slightly modified to take into account not only topics as sets of top-n words, but their probabilities in the distribution as well.\n",
    "\n",
    "In order to run this script, you will need to also download the pre-trained Word2Vec and Doc2Vec models that they use. See [this section](https://github.com/sb1992/NETL-Automatic-Topic-Labelling-#pre-trained-models) from their repository. These will need to be unzipped in the `netl_src/model_run/pre_trained_models/` directory.\n",
    "\n",
    "__NOTE__: This _will_ take a long time (a few hours, probably). There are 56 topics to process and a message will be printed every time one of them finishes, so you will know approximately how much is left at any moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src\n",
      "Extracting candidate labels\n",
      "Data Gathered\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/pre_trained_models/doc2vec/docvecmodel.d2v\n",
      "models loaded\n",
      "Done unique-ing indices\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:98: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:98: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "Done syn0norm\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:102: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.wv.syn0 = None\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:107: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n",
      "  model1_docvecs_doctag_syn0norm = (model1.docvecs.doctag_syn0 / sqrt((model1.docvecs.doctag_syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)[d_indices]\n",
      "doc2vec normalized\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:111: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model2.wv.syn0norm = (model2.wv.syn0 / sqrt((model2.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:111: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model2.wv.syn0norm = (model2.wv.syn0 / sqrt((model2.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:112: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model2.wv.syn0 = None\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:113: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model3 = model2.wv.syn0norm[w_indices]\n",
      "word2vec normalized\n",
      "Processing Topic number 0\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:140: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  tempdoc2vec = model1.wv.syn0norm[model1.wv.vocab[word].index] # The word2vec value of topic word from doc2vec trained model\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:152: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  tempword2vec = model2.wv.syn0norm[model2.wv.vocab[word].index]  # The word2vec value of topic word from word2vec trained model\n",
      "Processing Topic number 1\n",
      "Processing Topic number 2\n",
      "Processing Topic number 3\n",
      "Processing Topic number 4\n",
      "Processing Topic number 5\n",
      "Processing Topic number 6\n",
      "Processing Topic number 7\n",
      "Processing Topic number 8\n",
      "Processing Topic number 9\n",
      "Processing Topic number 10\n",
      "Processing Topic number 11\n",
      "Processing Topic number 12\n",
      "Processing Topic number 13\n",
      "Processing Topic number 14\n",
      "Processing Topic number 15\n",
      "Processing Topic number 16\n",
      "Processing Topic number 17\n",
      "Processing Topic number 18\n",
      "Processing Topic number 19\n",
      "Processing Topic number 20\n",
      "Processing Topic number 21\n",
      "Processing Topic number 22\n",
      "Processing Topic number 23\n",
      "Processing Topic number 24\n",
      "Processing Topic number 25\n",
      "Processing Topic number 26\n",
      "Processing Topic number 27\n",
      "Processing Topic number 28\n",
      "Processing Topic number 29\n",
      "Processing Topic number 30\n",
      "Processing Topic number 31\n",
      "Processing Topic number 32\n",
      "Processing Topic number 33\n",
      "Processing Topic number 34\n",
      "Processing Topic number 35\n",
      "Processing Topic number 36\n",
      "Processing Topic number 37\n",
      "Processing Topic number 38\n",
      "Processing Topic number 39\n",
      "Processing Topic number 40\n",
      "Processing Topic number 41\n",
      "Processing Topic number 42\n",
      "Processing Topic number 43\n",
      "Processing Topic number 44\n",
      "Processing Topic number 45\n",
      "Processing Topic number 46\n",
      "Processing Topic number 47\n",
      "Processing Topic number 48\n",
      "Processing Topic number 49\n",
      "Processing Topic number 50\n",
      "Processing Topic number 51\n",
      "Processing Topic number 52\n",
      "Processing Topic number 53\n",
      "Processing Topic number 54\n",
      "Processing Topic number 55\n",
      "Candidate labels written to ../experiment/netl_labels//output_candidates_biology\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ../netl_src/model_run/get_labels.py \\\n",
    "    --topics ${TOPICS_PATH}/biology.json \\\n",
    "    --output-dir ${NETL_LABELS_PATH} \\\n",
    "    --output-suffix biology \\\n",
    "    --candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the `../experiment/netl_labels/` directory should have the `output_candidates_biology` file. This containts the candidate labels selected by the NETL method that will be one source of labels (and the most important) used in fine-tuning the BART model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the BART dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, there are multiple options based on what kind of dataset we want to fine-tune BART on. The `BART-TL-ng` model showcased in the paper, for instance, was fine-tuned on a dataset created using the `bart-tl/build_dataset/terms_labels_ngrams/build_dataset_fairseq.py` script, while for `BART-TL-all` it was `bart-tl/build_dataset/terms_labels_sentences_ngrams_nps/build_dataset_fairseq.py`.\n",
    "\n",
    "Here we will fine-tune a `BART-TL-all` model for a more complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to generate the noun phrases for the topics, since unlike the sentences, these are not extracted when applying LDA in the first step.\n",
    "\n",
    "__NOTE__: This will also take a long amount of time, similar to the candidate selection at the previous step. After each completed topic, a message will be shown, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Done preamble.\n",
      "Done topic 0\n",
      "Done topic 1\n",
      "Done topic 2\n",
      "Done topic 3\n",
      "Done topic 4\n",
      "Done topic 5\n",
      "Done topic 7\n",
      "Done topic 8\n",
      "Done topic 10\n",
      "Done topic 12\n",
      "Done topic 13\n",
      "Done topic 14\n",
      "Done topic 16\n",
      "Done topic 17\n",
      "Done topic 19\n",
      "Done topic 20\n",
      "Done topic 21\n",
      "Done topic 22\n",
      "Done topic 23\n",
      "Done topic 24\n",
      "Done topic 25\n",
      "Done topic 26\n",
      "Done topic 27\n",
      "Done topic 28\n",
      "Done topic 29\n",
      "Done topic 32\n",
      "Done topic 35\n",
      "Done topic 36\n",
      "Done topic 39\n",
      "Done topic 42\n",
      "Done topic 43\n",
      "Done topic 46\n",
      "Done topic 48\n",
      "Done topic 52\n",
      "Done topic 53\n",
      "Done topic 56\n",
      "Done topic 58\n",
      "Done topic 59\n",
      "Done topic 60\n",
      "Done topic 61\n",
      "Done topic 63\n",
      "Done topic 68\n",
      "Done topic 70\n",
      "Done topic 72\n",
      "Done topic 74\n",
      "Done topic 75\n",
      "Done topic 76\n",
      "Done topic 80\n",
      "Done topic 81\n",
      "Done topic 82\n",
      "Done topic 84\n",
      "Done topic 87\n",
      "Done topic 93\n",
      "Done topic 94\n",
      "Done topic 95\n",
      "Done topic 97\n",
      "28696\n"
     ]
    }
   ],
   "source": [
    "!python3 ../lda/extract_noun_phrases.py \\\n",
    "    --lda-path ${LDA_INFO_PATH}/biology.model \\\n",
    "    --dict-path ${LDA_INFO_PATH}/biology.dct \\\n",
    "    --corpus-path ${LDA_INFO_PATH}/biology_corpus.pickle \\\n",
    "    --input-file ${CORPUS_FILE} \\\n",
    "    --output-prefix ${TOPICS_PATH}/biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../experiment/topics/` directory should now have the `biology_noun_phrases.txt` file with the extracted noun phrases. Now for creating the dataset for `BART-TL-all`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\r\n",
      "[nltk_data]   Package stopwords is already up-to-date!\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../bart-tl/build_dataset/terms_labels_sentences_ngrams_nps/build_dataset_fairseq.py \\\n",
    "    --topics ${TOPICS_PATH}/biology.json \\\n",
    "    --candidates-file ${NETL_LABELS_PATH}/output_candidates_biology \\\n",
    "    --sentences-file ${TOPICS_PATH}/biology_sentences_raw.txt \\\n",
    "    --noun-phrases-file ${TOPICS_PATH}/biology_noun_phrases.txt \\\n",
    "    --output-dir ${BART_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../experiment/dataset_fairseq/` directory should now contain 4 files:\n",
    "- `train.source`\n",
    "- `train.target`\n",
    "- `test.source`\n",
    "- `test.target`\n",
    "\n",
    "The `test` files are empty since we intend to use all the samples we have for fine-tuning BART.\n",
    "\n",
    "__IMPORTANT NOTE__: The script that creates the dataset does not overwrite the data in the `train.source` and `train.target` files. If you intend to fine-tune on more StackExchange corpora (as was done in the paper), you can run the script for each of them one after the other and the BART dataset will accumulate in the `train` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decide whether to use Huggingface or Fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to take a pause and decide the way to fine-tune the BART model on our dataset: __Huggingface__ or __Fairseq__.\n",
    "\n",
    "In the [original paper](https://www.aclweb.org/anthology/2021.eacl-main.121.pdf), I used [Facebook's Fairseq](https://github.com/pytorch/fairseq/tree/master/examples/bart) for fine-tuning. However, this is a rather difficult process (at least, more difficult than using Huggingface). You would need to pre-process the dataset further, download the [large BART model](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz) from them and have it saved locally, whereas the Huggingface method only requires running a single script with the current progress. Ultimately, I don't see a reason why using one over the other would yield wildly different results.\n",
    "\n",
    "I will showcase both methods here. If you are interested in getting results as similar to the original paper you can opt for the Fairseq way. On the other hand, if you want to quickly and easily use a model for topic labeling, you should go for Huggingface. Even moreso, if you don't want to fine-tune anything yourself, you can use the ones available on Huggingface:\n",
    "- https://huggingface.co/cristian-popa/bart-tl-all\n",
    "- https://huggingface.co/cristian-popa/bart-tl-ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Huggingface fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you will need two additional empty files in the dataset that are technically used for validation, but we won't do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"\" > ../experiment/dataset_fairseq/val.source\n",
    "!echo \"\" > ../experiment/dataset_fairseq/val.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, you will need some additional packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.1.1\n",
      "  Using cached transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (4.49.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (2020.11.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (2.25.1)\n",
      "Collecting tokenizers==0.9.4\n",
      "  Using cached tokenizers-0.9.4-cp39-cp39-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (1.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from transformers==4.1.1) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging->transformers==4.1.1) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.1.1) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.1.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.1.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.1.1) (4.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.1.1) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.1.1) (1.0.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from sacremoses->transformers==4.1.1) (7.1.2)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.2\n",
      "    Uninstalling tokenizers-0.10.2:\n",
      "      Successfully uninstalled tokenizers-0.10.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.4.2\n",
      "    Uninstalling transformers-4.4.2:\n",
      "      Successfully uninstalled transformers-4.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dsci-event-field-string-embedding 0.0.1 requires tokenizers==0.10.1, but you have tokenizers 0.9.4 which is incompatible.\n",
      "dsci-event-field-string-embedding 0.0.1 requires transformers==4.4.2, but you have transformers 4.1.1 which is incompatible.\u001b[0m\n",
      "Successfully installed tokenizers-0.9.4 transformers-4.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.9/site-packages (3.1.11)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/site-packages (from gitpython) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.9/site-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.9/site-packages (from rouge_score) (3.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from rouge_score) (0.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from rouge_score) (1.19.5)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/site-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/site-packages (from nltk->rouge_score) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/site-packages (from nltk->rouge_score) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from nltk->rouge_score) (4.49.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/site-packages (from nltk->rouge_score) (7.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.9/site-packages (1.4.14)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/site-packages (from sacrebleu) (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install \"transformers==4.1.1\"\n",
    "!pip install gitpython\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to successfully run the following script. It is originally from the [`transformers` repository](https://github.com/huggingface/transformers), but slightly modified so it doesn't break when the validation and test sets are empty files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../seq2seq/finetune_trainer.py\", line 367, in <module>\r\n",
      "    main()\r\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../seq2seq/finetune_trainer.py\", line 153, in main\r\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\r\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/hf_argparser.py\", line 52, in __init__\r\n",
      "    self._add_dataclass_arguments(dtype)\r\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/hf_argparser.py\", line 85, in _add_dataclass_arguments\r\n",
      "    elif hasattr(field.type, \"__origin__\") and issubclass(field.type.__origin__, List):\r\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py\", line 829, in __subclasscheck__\r\n",
      "    return issubclass(cls, self.__origin__)\r\n",
      "TypeError: issubclass() arg 1 must be a class\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../seq2seq/finetune_trainer.py \\\n",
    "    --model_name_or_path facebook/bart-large \\\n",
    "    --learning_rate=3e-5 \\\n",
    "    --do_train \\\n",
    "    --data_dir ${BART_DATASET_PATH} \\\n",
    "    --output_dir ${HUGGINGFACE_MODEL_SAVE_PATH} \\\n",
    "    --max_source_length 128 \\\n",
    "    --task summarization \\\n",
    "    --max_target_length 64 \\\n",
    "    --test_max_target_length 64 \\\n",
    "    --lr_scheduler polynomial \\\n",
    "    --logging_dir ${HUGGINGFACE_MODEL_SAVE_PATH} \\\n",
    "    --warmup_steps 98 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --adam_beta1 0.9 \\\n",
    "    --adam_beta2 0.999 \\\n",
    "    --adam_epsilon 1e-08 \\\n",
    "    --label_smoothing 0.1 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --run_name bart-tl-all-experiment \\\n",
    "    --save_steps 1626 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --max_grad_norm 0.1 \\\n",
    "    --dropout 0.1 \\\n",
    "    --attention_dropout 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the parameters above (in particular, set the `warmup_steps` to 6% of the total amount of steps) to fit your needs, these are the ones used in the paper. And that's it! After the fine-tuning is done, you will have all the data (and the model) in `../experiment/bart-tl-all/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate labels with the new `BART-TL-all` model, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nervous system' 'dna methylation' 'mammary gland' 'susceptibility'\n",
      "  'rna-seq' 'gene product' 'immune system' 'immune system' 'gene product'\n",
      "  'gene expression']\n",
      " ['glutathione' 'glycolysis' 'dopamine' 'glycerol' 'dopamine'\n",
      "  'fatty acid' 'glutamate receptor' 'bile acid' 'glycerol' 'metabolism']\n",
      " ['pulmonary hypertension' 'skeletal muscle' 'mitochondrion'\n",
      "  'pericardium' 'thrombosis' 'apoptosis' 'breathing system'\n",
      "  'pulmonary valve' 'breathing tube' 'heterologous']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = '../experiment/bart-tl-all/'\n",
    "num_labels_to_generate = 10\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# If you want to use GPU, uncomment this line:\n",
    "# model = model.to('cuda')\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Remember: the model is only trained on the biology corpus!\n",
    "texts = [\n",
    "    'virus vaccine influenza infection vaccination pig human hpv disease antibody',\n",
    "    'diabetes glucose insulin type diabetic cholesterol level lipoprotein control lipid',\n",
    "    'cardiac heart ventricular patient myocardial failure valve atrial left leave'\n",
    "]\n",
    "\n",
    "batch = tokenizer(texts, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "generated_labels = model.generate(\n",
    "    input_ids=batch.input_ids,\n",
    "    attention_mask=batch.attention_mask,\n",
    "    max_length=15,\n",
    "    min_length=1,\n",
    "    do_sample=False,\n",
    "    num_beams=25,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.5,\n",
    "    num_return_sequences=num_labels_to_generate\n",
    ")\n",
    "\n",
    "generated_labels = tokenizer.batch_decode(generated_labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "generated_labels = np.array(generated_labels).reshape((len(batch.input_ids), num_labels_to_generate))\n",
    "\n",
    "print(generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.6.0\n",
      "  Using cached torch-1.6.0-cp38-none-macosx_10_9_x86_64.whl (97.5 MB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/site-packages (from torch==1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from torch==1.6.0) (1.18.5)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "Successfully installed torch-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!python3.8 -m pip install torch==1.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Fairseq fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataset, it needs to be processed further in the case of Fairseq.\n",
    "\n",
    "First of all, you need to install the `fairseq` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairseq==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset need BPE preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python@3.9/3.9.1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 60 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/usr/local/Cellar/python@3.9/3.9.1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 60 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/usr/local/Cellar/python@3.9/3.9.1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 56 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "!../bart-tl/preprocess/bpe/bpe_preprocess.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create more files in the `../experiment/dataset_fairseq/` directory.\n",
    "\n",
    "Afterwards, the dataset needs to be binarized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', task='translation', source_lang='source', target_lang='target', trainpref='../experiment/dataset_fairseq/train.bpe', validpref='../experiment/dataset_fairseq/test.bpe', testpref=None, align_suffix=None, destdir='../experiment/dataset_fairseq-bin/', thresholdtgt=0, thresholdsrc=0, tgtdict='../bart-tl/preprocess/bpe/dict.txt', srcdict='../bart-tl/preprocess/bpe/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=60)\n",
      "| [source] Dictionary: 50263 types\n",
      "| [source] ../experiment/dataset_fairseq/train.bpe.source: 6498 sents, 219895 tokens, 0.0% replaced by <unk>\n",
      "| [source] Dictionary: 50263 types\n",
      "| [source] ../experiment/dataset_fairseq/test.bpe.source: 1 sents, 1 tokens, 0.0% replaced by <unk>\n",
      "| [target] Dictionary: 50263 types\n",
      "| [target] ../experiment/dataset_fairseq/train.bpe.target: 6498 sents, 39197 tokens, 0.0% replaced by <unk>\n",
      "| [target] Dictionary: 50263 types\n",
      "| [target] ../experiment/dataset_fairseq/test.bpe.target: 1 sents, 1 tokens, 0.0% replaced by <unk>\n",
      "| Wrote preprocessed data to ../experiment/dataset_fairseq-bin/\n"
     ]
    }
   ],
   "source": [
    "!../bart-tl/preprocess/binarization/binarize.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create another directory, `../experiment/dataset_fairseq-bin/` with the final version of the data that will be fed to the BART model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now need to download the `bart.large` model from the Fairseq repository: https://github.com/pytorch/fairseq/blob/master/examples/bart/README.md#pre-trained-models.\n",
    "\n",
    "Unzip it into the `../experiment/` directory. You will now have a directory named `../experiment/bart.large/` that contains a `model.pt` file. To fine-tune the model, you need to run the following cell.\n",
    "\n",
    "__IMPORTANT NOTE__: The script does not actually ever end, you need to stop it manually after 2 epochs are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(no_progress_bar=False, log_interval=1000, log_format=None, tensorboard_logdir='', seed=1, cpu=False, fp16=False, memory_efficient_fp16=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=True, max_tokens=2048, max_sentences=None, required_batch_size_multiple=1, dataset_impl=None, train_subset='train', valid_subset='valid', validate_interval=1, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=2048, max_sentences_valid=None, curriculum=0, distributed_world_size=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, arch='bart_large', max_epoch=0, max_update=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[3e-05], min_lr=-1, use_bmuf=False, save_dir='checkpoints', restore_file='../experiment/bart.large/model.pt', reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=True, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, layer_wise_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, label_smoothing=0.1, adam_betas='(0.9, 0.999)', adam_eps=1e-08, weight_decay=0.01, force_anneal=None, warmup_updates=4, end_learning_rate=0.0, power=1.0, total_num_update=64, data='../experiment/dataset_fairseq-bin', source_lang='source', target_lang='target', lazy_load=False, raw_text=False, load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=True, layernorm_embedding=True, share_all_embeddings=True, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, encoder_embed_path=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_layers=12, encoder_attention_heads=16, encoder_normalize_before=False, encoder_learned_pos=True, decoder_embed_path=None, decoder_embed_dim=1024, decoder_ffn_embed_dim=4096, decoder_layers=12, decoder_attention_heads=16, decoder_normalize_before=False, decoder_learned_pos=True, relu_dropout=0.0, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, decoder_output_dim=1024, decoder_input_dim=1024, no_scale_embedding=True, activation_fn='gelu', pooler_activation_fn='tanh', pooler_dropout=0.0)\n",
      "| [source] dictionary: 50264 types\n",
      "| [target] dictionary: 50264 types\n",
      "| loaded 1 examples from: ../experiment/dataset_fairseq-bin/valid.source-target.source\n",
      "| loaded 1 examples from: ../experiment/dataset_fairseq-bin/valid.source-target.target\n",
      "| ../experiment/dataset_fairseq-bin valid source-target 1 examples\n",
      "BARTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
      "    (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (4): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (5): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (6): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (7): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (8): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (9): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (10): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "      (11): TransformerDecoderLayer(\r\n",
      "        (self_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (encoder_attn): MultiheadAttention(\r\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\r\n",
      "        )\r\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\r\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\r\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\r\n",
      "  )\r\n",
      "  (classification_heads): ModuleDict()\r\n",
      ")\r\n",
      "| model bart_large, criterion LabelSmoothedCrossEntropyCriterion\r\n",
      "| num. model params: 406290432 (num. trained: 406290432)\r\n",
      "| training on 1 GPUs\r\n",
      "| max tokens per GPU = 2048 and max sentences per GPU = None\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loaded checkpoint ../experiment/bart.large/model.pt (epoch 41 @ 0 updates)\n",
      "| loading train data for epoch 0\n",
      "| loaded 6498 examples from: ../experiment/dataset_fairseq-bin/train.source-target.source\n",
      "| loaded 6498 examples from: ../experiment/dataset_fairseq-bin/train.source-target.target\n",
      "| ../experiment/dataset_fairseq-bin train source-target 6498 examples\n",
      "| epoch 001:   0%|                                      | 0/128 [00:00<?, ?it/s]/usr/local/lib/python3.9/site-packages/fairseq/optim/adam.py:160: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "| epoch 001 | loss 8.438 | nll_loss 6.990 | ppl 127.11 | wps 7 | ups 0 | wpb 306.227 | bsz 50.766 | num_updates 128 | lr 0 | gnorm 116.923 | clip 1.000 | oom 0.000 | wall 5356 | train_wall 5310\n",
      "| epoch 001 | valid on 'valid' subset | loss 15.172 | nll_loss 14.692 | ppl 26464.8 | num_updates 128\n",
      "| saved checkpoint checkpoints/checkpoint1.pt (epoch 1 @ 128 updates) (writing took 28.25100016593933 seconds)\n",
      "| epoch 002 | loss 7.779 | nll_loss 6.264 | ppl 76.83 | wps 8 | ups 0 | wpb 306.227 | bsz 50.766 | num_updates 256 | lr 0 | gnorm 78.137 | clip 1.000 | oom 0.000 | wall 10364 | train_wall 10245\n",
      "| epoch 002 | valid on 'valid' subset | loss 15.172 | nll_loss 14.692 | ppl 26464.8 | num_updates 256 | best_loss 15.1716\n",
      "| saved checkpoint checkpoints/checkpoint2.pt (epoch 2 @ 256 updates) (writing took 36.882728099823 seconds)\n",
      "| epoch 003:  22%|| 28/128 [15:49<53:00, 31.80s/it, loss=7.532, nll_loss=5.989,^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../bart-tl/finetune/train.py\", line 11, in <module>\n",
      "    cli_main()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq_cli/train.py\", line 333, in cli_main\n",
      "    main(args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq_cli/train.py\", line 86, in main\n",
      "    train(args, trainer, task, epoch_itr)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq_cli/train.py\", line 127, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/trainer.py\", line 437, in train_step\n",
      "    self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/optim/fairseq_optimizer.py\", line 98, in step\n",
      "    self.optimizer.step(closure)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/optim/adam.py\", line 160, in step\n",
      "    exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!../bart-tl/finetune/finetune_bart.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model checkpoints should be available in the `notebooks/checkpoints/` directory and, if you stopped the cell after 2 epochs were finished, you will have the following files:\n",
    "- `checkpoint1.pt`\n",
    "- `checkpoint2.pt`\n",
    "- `checkpoint_best.pt`\n",
    "\n",
    "You only need to keep the `checkpoint2.pt` file, as that's the fine-tuned model after 2 epochs.\n",
    "\n",
    "To generate labels using it, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading archive file .\n",
      "| [source] dictionary: 50264 types\n",
      "| [target] dictionary: 50264 types\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../bart-tl/generate.py\", line 142, in <module>\n",
      "    main()\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../bart-tl/generate.py\", line 122, in main\n",
      "    hypotheses_batch.append(bart.sample([topic], num_samples=num_samples, beam=beam, lenpen=2.0, max_len_b=60, min_len=1, no_repeat_ngram_size=10, length_penalty=1.0)[0])\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../bart-tl/generate.py\", line 46, in sample\n",
      "    sample, translations = self.generate(input, beam, verbose, **kwargs)\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../bart-tl/generate.py\", line 67, in generate\n",
      "    translations = self.task.inference_step(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/tasks/fairseq_task.py\", line 265, in inference_step\n",
      "    return generator.generate(models, sample, prefix_tokens=prefix_tokens)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/sequence_generator.py\", line 113, in generate\n",
      "    return self._generate(model, sample, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/sequence_generator.py\", line 376, in _generate\n",
      "    cand_scores, cand_indices, cand_beams = self.search.step(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/fairseq/search.py\", line 81, in step\n",
      "    torch.div(self.indices_buf, vocab_size, out=self.beams_buf)\n",
      "RuntimeError: result type Float can't be cast to the desired output type Long\n"
     ]
    }
   ],
   "source": [
    "!python3 ../bart-tl/generate.py \\\n",
    "    --model-path checkpoints/checkpoint2.pt \\\n",
    "    --processed-dataset-path ../experiment/dataset_fairseq-bin/ \\\n",
    "    --topics-file ../experiment/dataset_fairseq/train.source \\\n",
    "    --output-file ../experiment/generated_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I was not able to get this generation to work, even though the code was the exact same as what I used in experimenting (most probably, some packages got messed up in the meantime). Sorry about this :(\n",
    "\n",
    "This _should_ generate labels for all the topics in the training file and put them in `../experiment/generated_labels.txt`. The labels are space-separated and each individual one has spaces replaced by `_` characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
