{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End-to-end Training of BART-TL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must first install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.0.1 in /usr/local/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (4.0.1)\n",
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/site-packages (from gensim==4.0.1->-r ../requirements.txt (line 1)) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/site-packages (from gensim==4.0.1->-r ../requirements.txt (line 1)) (1.6.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Applying LDA on corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is extracting topics from the StackExchange corpora. In this notebook we only experiment with the biology corpus. Using the other corpora is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CORPUS_FILE=../corpus/biology.stackexchange.com/Posts.xml\n",
      "env: LDA_INFO_PATH=../experiment/lda_info/\n",
      "env: TOPICS_PATH=../experiment/topics/\n",
      "env: NETL_LABELS_PATH=../experiment/netl_labels/\n",
      "env: BART_DATASET_PATH=../experiment/dataset_fairseq/\n",
      "env: HUGGINGFACE_MODEL_SAVE_PATH=../experiment/bart-tl-all/\n"
     ]
    }
   ],
   "source": [
    "# XML file used in the experiment\n",
    "%env CORPUS_FILE=../corpus/biology.stackexchange.com/Posts.xml\n",
    "# directory where the LDA models and info will be stored\n",
    "%env LDA_INFO_PATH=../experiment/lda_info/\n",
    "# directory where the topic data will be stored\n",
    "%env TOPICS_PATH=../experiment/topics/\n",
    "# directory where the NETL-extracted labels will be stored\n",
    "%env NETL_LABELS_PATH=../experiment/netl_labels/\n",
    "\n",
    "# directory where the dataset for fine-tuning BART will be stored\n",
    "%env BART_DATASET_PATH=../experiment/dataset_fairseq/\n",
    "# where the model will be saved\n",
    "%env HUGGINGFACE_MODEL_SAVE_PATH=../experiment/bart-tl-all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ${LDA_INFO_PATH}\n",
    "!mkdir -p ${TOPICS_PATH}\n",
    "!mkdir -p ${NETL_LABELS_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the script that applies LDA on the biology corpus from StackExchange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Done pre-processing documents\n",
      "Done pre-processing corpus\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python3 ../lda/apply_lda.py \\\n",
    "    --input-file ${CORPUS_FILE} \\\n",
    "    --output-prefix ${LDA_INFO_PATH}/biology \\\n",
    "    --topics-prefix ${TOPICS_PATH}/biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the `../experiment/lda_info` directory should contain the following files:\n",
    "- `biology.dct`\n",
    "- `biology.model`\n",
    "- `biology.model.expElogbeta.npy`\n",
    "- `biology.model.id2word`\n",
    "- `biology.model.state`\n",
    "- `biology_corpus.pickle`\n",
    "\n",
    "These files are useful for extracting the noun phrases at step 3 and are saved so you can better inspect LDA-related issues for experiments.\n",
    "\n",
    "The `../experiment/topics` directory will contain:\n",
    "- `biology.csv`\n",
    "- `biology.json`\n",
    "- `biology_sentences.txt`\n",
    "- `biology_sentences_raw.txt`\n",
    "\n",
    "These are files that will be used to generate datasets the BART model will be fine-tuned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining NETL labels for topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the topics from the corpus, these need to be labeled using the NETL method (https://github.com/sb1992/NETL-Automatic-Topic-Labelling-).\n",
    "\n",
    "The original process proposed by the authors was slightly modified to take into account not only topics as sets of top-n words, but their probabilities in the distribution as well.\n",
    "\n",
    "In order to run this script, you will need to also download the pre-trained Word2Vec and Doc2Vec models that they use. See [this section](https://github.com/sb1992/NETL-Automatic-Topic-Labelling-#pre-trained-models) from their repository. These will need to be unzipped in the `netl_src/model_run/pre_trained_models/` directory.\n",
    "\n",
    "__NOTE__: This _will_ take a long time (a few hours, probably). There are 56 topics to process and a message will be printed every time one of them finishes, so you will know approximately how much is left at any moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src\n",
      "Extracting candidate labels\n",
      "Data Gathered\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/pre_trained_models/doc2vec/docvecmodel.d2v\n",
      "models loaded\n",
      "Done unique-ing indices\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:98: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:98: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model1.wv.syn0norm = (model1.wv.syn0 / sqrt((model1.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "Done syn0norm\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:102: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model1.wv.syn0 = None\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:107: DeprecationWarning: Call to deprecated `doctag_syn0` (Attribute will be removed in 4.0.0, use docvecs.vectors_docs instead).\n",
      "  model1_docvecs_doctag_syn0norm = (model1.docvecs.doctag_syn0 / sqrt((model1.docvecs.doctag_syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)[d_indices]\n",
      "doc2vec normalized\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:111: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model2.wv.syn0norm = (model2.wv.syn0 / sqrt((model2.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:111: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model2.wv.syn0norm = (model2.wv.syn0 / sqrt((model2.wv.syn0 ** 2).sum(-1))[..., newaxis]).astype(REAL)\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:112: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  model2.wv.syn0 = None\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:113: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  model3 = model2.wv.syn0norm[w_indices]\n",
      "word2vec normalized\n",
      "Processing Topic number 0\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:140: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  tempdoc2vec = model1.wv.syn0norm[model1.wv.vocab[word].index] # The word2vec value of topic word from doc2vec trained model\n",
      "/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../netl_src/model_run/cand_generation.py:152: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  tempword2vec = model2.wv.syn0norm[model2.wv.vocab[word].index]  # The word2vec value of topic word from word2vec trained model\n",
      "Processing Topic number 1\n",
      "Processing Topic number 2\n",
      "Processing Topic number 3\n",
      "Processing Topic number 4\n",
      "Processing Topic number 5\n",
      "Processing Topic number 6\n",
      "Processing Topic number 7\n",
      "Processing Topic number 8\n",
      "Processing Topic number 9\n",
      "Processing Topic number 10\n",
      "Processing Topic number 11\n",
      "Processing Topic number 12\n",
      "Processing Topic number 13\n",
      "Processing Topic number 14\n",
      "Processing Topic number 15\n",
      "Processing Topic number 16\n",
      "Processing Topic number 17\n",
      "Processing Topic number 18\n",
      "Processing Topic number 19\n",
      "Processing Topic number 20\n",
      "Processing Topic number 21\n",
      "Processing Topic number 22\n",
      "Processing Topic number 23\n",
      "Processing Topic number 24\n",
      "Processing Topic number 25\n",
      "Processing Topic number 26\n",
      "Processing Topic number 27\n",
      "Processing Topic number 28\n",
      "Processing Topic number 29\n",
      "Processing Topic number 30\n",
      "Processing Topic number 31\n",
      "Processing Topic number 32\n",
      "Processing Topic number 33\n",
      "Processing Topic number 34\n",
      "Processing Topic number 35\n",
      "Processing Topic number 36\n",
      "Processing Topic number 37\n",
      "Processing Topic number 38\n",
      "Processing Topic number 39\n",
      "Processing Topic number 40\n",
      "Processing Topic number 41\n",
      "Processing Topic number 42\n",
      "Processing Topic number 43\n",
      "Processing Topic number 44\n",
      "Processing Topic number 45\n",
      "Processing Topic number 46\n",
      "Processing Topic number 47\n",
      "Processing Topic number 48\n",
      "Processing Topic number 49\n",
      "Processing Topic number 50\n",
      "Processing Topic number 51\n",
      "Processing Topic number 52\n",
      "Processing Topic number 53\n",
      "Processing Topic number 54\n",
      "Processing Topic number 55\n",
      "Candidate labels written to ../experiment/netl_labels//output_candidates_biology\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 ../netl_src/model_run/get_labels.py \\\n",
    "    --topics ${TOPICS_PATH}/biology.json \\\n",
    "    --output-dir ${NETL_LABELS_PATH} \\\n",
    "    --output-suffix biology \\\n",
    "    --candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the script, the `../experiment/netl_labels/` directory should have the `output_candidates_biology` file. This containts the candidate labels selected by the NETL method that will be one source of labels (and the most important) used in fine-tuning the BART model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating the BART dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, there are multiple options based on what kind of dataset we want to fine-tune BART on. The `BART-TL-ng` model showcased in the paper, for instance, was fine-tuned on a dataset created using the `bart-tl/build_dataset/terms_labels_ngrams/build_dataset_fairseq.py` script, while for `BART-TL-all` it was `bart-tl/build_dataset/terms_labels_sentences_ngrams_nps/build_dataset_fairseq.py`.\n",
    "\n",
    "Here we will fine-tune a `BART-TL-all` model for a more complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to generate the noun phrases for the topics, since unlike the sentences, these are not extracted when applying LDA in the first step.\n",
    "\n",
    "__NOTE__: This will also take a long amount of time, similar to the candidate selection at the previous step. After each completed topic, a message will be shown, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Done preamble.\n",
      "Done topic 0\n",
      "Done topic 1\n",
      "Done topic 2\n",
      "Done topic 3\n",
      "Done topic 4\n",
      "Done topic 5\n",
      "Done topic 7\n",
      "Done topic 8\n",
      "Done topic 10\n",
      "Done topic 12\n",
      "Done topic 13\n",
      "Done topic 14\n",
      "Done topic 16\n",
      "Done topic 17\n",
      "Done topic 19\n",
      "Done topic 20\n",
      "Done topic 21\n",
      "Done topic 22\n",
      "Done topic 23\n",
      "Done topic 24\n",
      "Done topic 25\n",
      "Done topic 26\n",
      "Done topic 27\n",
      "Done topic 28\n",
      "Done topic 29\n",
      "Done topic 32\n",
      "Done topic 35\n",
      "Done topic 36\n",
      "Done topic 39\n",
      "Done topic 42\n",
      "Done topic 43\n",
      "Done topic 46\n",
      "Done topic 48\n",
      "Done topic 52\n",
      "Done topic 53\n",
      "Done topic 56\n",
      "Done topic 58\n",
      "Done topic 59\n",
      "Done topic 60\n",
      "Done topic 61\n",
      "Done topic 63\n",
      "Done topic 68\n",
      "Done topic 70\n",
      "Done topic 72\n",
      "Done topic 74\n",
      "Done topic 75\n",
      "Done topic 76\n",
      "Done topic 80\n",
      "Done topic 81\n",
      "Done topic 82\n",
      "Done topic 84\n",
      "Done topic 87\n",
      "Done topic 93\n",
      "Done topic 94\n",
      "Done topic 95\n",
      "Done topic 97\n",
      "28696\n"
     ]
    }
   ],
   "source": [
    "!python3 ../lda/extract_noun_phrases.py \\\n",
    "    --lda-path ${LDA_INFO_PATH}/biology.model \\\n",
    "    --dict-path ${LDA_INFO_PATH}/biology.dct \\\n",
    "    --corpus-path ${LDA_INFO_PATH}/biology_corpus.pickle \\\n",
    "    --input-file ${CORPUS_FILE} \\\n",
    "    --output-prefix ${TOPICS_PATH}/biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../experiment/topics/` directory should now have the `biology_noun_phrases.txt` file with the extracted noun phrases. Now for creating the dataset for `BART-TL-all`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cpopa/nltk_data...\r\n",
      "[nltk_data]   Package stopwords is already up-to-date!\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../bart-tl/build_dataset/terms_labels_sentences_ngrams_nps/build_dataset_fairseq.py \\\n",
    "    --topics ${TOPICS_PATH}/biology.json \\\n",
    "    --candidates-file ${NETL_LABELS_PATH}/output_candidates_biology \\\n",
    "    --sentences-file ${TOPICS_PATH}/biology_sentences_raw.txt \\\n",
    "    --noun-phrases-file ${TOPICS_PATH}/biology_noun_phrases.txt \\\n",
    "    --output-dir ${BART_DATASET_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `../experiment/dataset_fairseq/` directory should now contain 4 files:\n",
    "- `train.source`\n",
    "- `train.target`\n",
    "- `test.source`\n",
    "- `test.target`\n",
    "\n",
    "The `test` files are empty since we intend to use all the samples we have for fine-tuning BART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decide whether to use Huggingface or Fairseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to take a pause and decide the way to fine-tune the BART model on our dataset: __Huggingface__ or __Fairseq__.\n",
    "\n",
    "In the [original paper](https://www.aclweb.org/anthology/2021.eacl-main.121.pdf), I used [Facebook's Fairseq](https://github.com/pytorch/fairseq/tree/master/examples/bart) for fine-tuning. However, this is a rather difficult process (at least, more difficult than using Huggingface). You would need to pre-process the dataset further, download the [large BART model](https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz) from them and have it saved locally, whereas the Huggingface method only requires running a single script with the current progress. Ultimately, I don't see a reason why using one over the other would yield wildly different results.\n",
    "\n",
    "I will showcase both methods here. If you are interested in getting results as similar to the original paper you can opt for the Fairseq way. On the other hand, if you want to quickly and easily use a model for topic labeling, you should go for Huggingface - and I will soon update the model on https://huggingface.co/models, so you don't even need to fine-tune the model yourself (or go through any of the previous steps altogether, in fact)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Huggingface fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"\" > ../experiment/dataset_fairseq/val.source\n",
    "!echo \"\" > ../experiment/dataset_fairseq/val.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to clone the [`transformers` repository](https://github.com/huggingface/transformers) for the following fine-tuning script to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../seq2seq/finetune_trainer.py\", line 367, in <module>\r\n",
      "    main()\r\n",
      "  File \"/Users/cpopa/personal/workspace/bart-tl-topic-label-generation/notebooks/../seq2seq/finetune_trainer.py\", line 153, in main\r\n",
      "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\r\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/hf_argparser.py\", line 52, in __init__\r\n",
      "    self._add_dataclass_arguments(dtype)\r\n",
      "  File \"/usr/local/lib/python3.9/site-packages/transformers/hf_argparser.py\", line 85, in _add_dataclass_arguments\r\n",
      "    elif hasattr(field.type, \"__origin__\") and issubclass(field.type.__origin__, List):\r\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/typing.py\", line 829, in __subclasscheck__\r\n",
      "    return issubclass(cls, self.__origin__)\r\n",
      "TypeError: issubclass() arg 1 must be a class\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../seq2seq/finetune_trainer.py \\\n",
    "    --model_name_or_path facebook/bart-large \\\n",
    "    --learning_rate=3e-5 \\\n",
    "    --do_train \\\n",
    "    --data_dir ${BART_DATASET_PATH} \\\n",
    "    --output_dir ${HUGGINGFACE_MODEL_SAVE_PATH} \\\n",
    "    --max_source_length 128 \\\n",
    "    --task summarization \\\n",
    "    --max_target_length 64 \\\n",
    "    --test_max_target_length 64 \\\n",
    "    --lr_scheduler polynomial \\\n",
    "    --logging_dir ${HUGGINGFACE_MODEL_SAVE_PATH} \\\n",
    "    --warmup_steps 1027 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --adam_beta1 0.9 \\\n",
    "    --adam_beta2 0.999 \\\n",
    "    --adam_epsilon 1e-08 \\\n",
    "    --label_smoothing 0.1 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --run_name bart-tl-all-experiment \\\n",
    "    --save_steps 42012 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --max_grad_norm 0.1 \\\n",
    "    --dropout 0.1 \\\n",
    "    --attention_dropout 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! After the fine-tuning is done, you will have all the data (and the model) in `../experiment/bart-tl-all/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate labels with the new `BART-TL-all` model, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = '../experiment/bart-tl-all/'\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# If you want to use GPU, uncomment this line\n",
    "# model = model.to('cuda')\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = 'business company technology product customer service provide management development system'\n",
    "\n",
    "batch = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "generated_labels = model.generate(\n",
    "    input_ids=batch.input_ids,\n",
    "    attention_mask=batch.attention_mask,\n",
    "    max_length=15,\n",
    "    min_length=1,\n",
    "    do_sample=False,\n",
    "    num_beams=25,\n",
    "    length_penalty=1.0,\n",
    "    repetition_penalty=1.5,\n",
    "    num_return_sequences=10\n",
    ")\n",
    "\n",
    "print('Generated labels: ' + ', '.join(generated_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Fairseq fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataset, it needs to be processed further in the case of Fairseq.\n",
    "\n",
    "First of all, it needs BPE preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../bart-tl/preprocess/bpe/bpe_preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Afterwards, it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
